---
layout: post
title:  "Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution (ICLR 2022 Oral)"
date:   2023-07-23 08:00:00
author: VennTum
tags: [AI, deep-learning]
---

# [Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution](https://arxiv.org/abs/2202.10054)

최근 들어서 굉장히 많은 딥러닝 영역에서 대규모 pretrained model을 특정한 downstream task에 대해 fine-tuning 하는 방식으로 학습을 진행하는 경우가 많습니다.
이전에는 데이터 셋의 규모가 작고 지금과 같이 transformer 구조를 사용하지 않을 때에는 요즘과 같이 large pretrained model이 크게 유행하지 않았습니다. 이전에는 많은 경우에 ImageNet 정도 사이즈의 데이터 셋에서 학습한 pretraiend model의 parameter를 가져와서 이보다도 더 작은 downstream task로 fine-tuning 하였기에 특정 task들에 대해서는 transfer learning을 적용해도 성능이 향상되지 않은 경우들도 있었으며, 모델 구조 자체가 대규모 학습에 용이하지 않은 경우도 있었습니다.

그러나 현재에는 거의 모든 분야에서 transformer 구조가 굉장히 큰 확장성과 대규모 학습을 처리해도 성능이 떨어지지 않는다는 점, 그리고 대규모 학습을 진행한 경우, 웬만한 downstream task에 transfer learning을 적용하였을 때에 성능이 향상한다는 점들이 알려지며, 거의 모든 분야에서 large pretrained model을 사용하는 방식으로 변화하였습니다.
이로 인해 CLIP, BERT 등 다양한 pretrained model을 사용하는 것이 굉장히 유용하고 효율적으로 문제를 해결할 수 있게 됩니다.

그 결과, 최근 양상은 이러한 pretrained model을 어떻게하면 특정 downstream task에 잘 적용할 수 있는지에 대해 중점적으로 연구되고 있습니다. 이러한 대규모 학습이 필요한 모델들의 경우, 실제로 한 번 학습하는 것에 굉장히 오랜 자원들이 필요하기에, 한 번 잘 학습시켜놓고 이를 잘 적용시키는 방향으로 많은 부분이 연구되고 있습니다.
실제로 제가 이전에 포스팅한 WiSE-FT의 경우도 이러한 large pretrained model을 downstream task에 적용하는 과정에서 fine-tuning을 한 이후 target distribution과 distribution shift case에 대한 두 가지 성능을 모두 끌어올리는 새로운 방법을 제안하기도 했습니다.

이번에 소개할 논문인 Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution도 이러한 transfer learning을 진행하는 과정에서 어떻게 더 효율적으로 성능을 끌어올릴 수 있는지에 대해 연구하였습니다. 그러나 이번 논문에서는 target이 완전히 large pretrained model에만 국한된 것은 아니며, 아예 transfer learning 자체를 효율적으로 할 수 있는 방법을 제안합니다.

해당 논문에서 소개하는 방법론은 어렵지 않으면서도 transfer learning이 진행되는 과정과 이를 통해 발생하는 문제를 해결하는 과정을 굉장히 잘 visualize하고, 이를 바탕으로 굉장히 쉽고 효율적으로 transfer learning을 진행하는 방법을 제안합니다. Transfer learning이 필요한 영역에서 누구나 쉽게 아주 약간의 코드 수정만을 통해 해당 기법을 적용할 수 있습니다.
이 논문은 이러한 점에서 참신함과 효율을 인정받아 ICLR 2022에서 Oral presentation을 진행할 수 있었습니다.

## Introduction

Fine-tuning에는 크게 두 가지 방법이 있는 것이 잘 알려져있습니다. 실제로 지금 제가 fine-tuning이라고 두 개를 모두 통합하여 transfer learning 과정에서 pretrained model을 downstream task 데이터 셋에 대해 추가로 학습시키는 것을 이야기하고 있습니다. 그러나 실제로는 linear probing이라는 방법과 end-to-end fine-tuning이라는 서로 다른 fine-tuning 기법이 존재하며 이 둘이 보여주는 결과 양상도 상당히 다릅니다.

- Linear probing: Pretrained model의 output 단에 레이어를 추가하여 downstream task를 위한 모델로 만든 이후, 기존의 pretrained model의 layer들은 모두 freeze하여 학습되지 않도록 한 이후, downstream task를 학습하면서 새롭게 추가한 output layer만 학습시키는 것
- End-to-end fine-tuning: Linear probing과 동일한 모델 셋팅을 사용하나, 학습된 pretrain model layer들도 trainable하게 셋팅하여 downstream task를 추가로 학습하는 것

이 둘은 논문마다 명칭을 다르게 사용하기도 합니다만, 본 포스트에서는 각각 linear probing과 fine-tuning이라는 명칭을 사용해서 부르도록 하겠습니다.
(실제로 WiSE-FT 논문에서는 이 둘을 linear classifier와 end-to-end로 구분하여 불렀습니다)


이 둘이 차별 양상을 보이는 가장 큰 부분은 바로 downstream task에 대해 학습했을 때의 성능과 distribution shift dataset에 대한 성능입니다.
보편적으로 알려져있는 결과는 fine-tuning의 경우, 해당하는 target distribution과 이와 유시한 small distribution shift들에 대해서는 linear probing보다 높은 성능을 보여준다는 것이고, 반대로 linear probing의 경우 down stream task의 target distribution과 상당히 먼 large distribution shift들에 대해서 fine-tuning에 비해 높은 성능을 보인다는 것입니다.

즉, linear probing은 OOD case에 대해 robustness를 잘 가지고 있으며, fine-tuning은 ID case에 대해 높은 성능을 보여줍니다.
이는 실제 해당 논문에서 실험하는 데이터 셋들 뿐만아니라, 많은 경우에도 이러한 양상을 잘 보여줍니다.
실제로 저번에 포스팅한 WiSE-FT에서도 linear probing의 경우 ImageNet을 target distribution으로 했을 때, 이와 유사한 ImageNetV2 등에 대해서는 fine-tuning보다 낮은 성능을 보였으나, distribution shift가 강한 ObjectNet, IN-A 등에 대해서는 fine-tuning보다 월등한 성능을 보였습니다.

이에 해당 논문에서는 이렇게 linear probing과 fine-tuning이 distribution shift case에 대해서 성능 차이가 나는 이유에 대해 분석하고, 이를 개선하여 fine-tuning과 linear probing의 장점을 합한 새로운 fine-tuning 기법인 LP-FT를 제안합니다.

## Theory

### Linear overparameterized setting

저자들은 어떠한 condition에서 fine-tuning이 linear probing보다 OOD에 약한지 확인하기 위해 pretrained feature가 해당 target에 대해 좋은 성능을 보이며, 이에 대한 OOD shift가 큰 linear setting에 대해 실험합니다.

이를 위해 특정 데이터 셋에 대해 L2 norm loss를 사용하는 regression case를 테스트합니다.

위 상황을 해결하는 모델의 feature extractor를 B, B' 이라 할 때, 해당 feature extractor들 사이의 distance는 다음과 같이 정의됩니다.

- Feature Extractor Distance: $d(B, B') = min_{U} ||B - UB'||_{2}, where U is rotation matrix, B, B' are orthonormal$

이와 같이 정의된 feature extractor distance에 대해, 좋은 성능을 보이는 Good pretrained feature $B{0}는 특정한 오차 \epsilon과 optimal feature extractor $B^{*}$에 대해 $d(B{0}, B^{*} <= \epsilon$인 경우를 이야기합니다.

이러한 상황에서, 해당 distribution들에서 유용한 feature set을 공유하고 있는 ID와 OOD가 존재한다고 할 때, pretrained model이 학습 과정에서 해당하는 large dataset에 대해 학습하는 과정에서 각각의 downstream task의 ID와 OOD에 대한 unlabeled 혹은 weak supervised learning이 되었을 경우, $B_{0}가 B^{*}$와 가깝다고 가정할 수 있도록 학습된 상황이 일어날 수 있습니다.

**저자들은 이러한 B0가 존재하는 상황에서도, fine-tuning이 이를 distort하여 낮은 OOD accuracy를 만들 수 있다는 것을 증명합니다.**

이외에 더 자세한 세팅 디테일에 대해서는 논문에서 확인하실 수 있습니다.

이제 실제로 이러한 상황 속에서 어떠한 결과를 저자들이 확인하였는지 살펴보도록 하겠습니다.

저자들은 위와 같은 configuration 속에서, 각각의 pretrained feature extractor들을 해당하는 ID에 대해 linear probing(LP)와 fine-tuning(FT)를 사용하여 학습합니다.
그리고 이와 같이 학습된 각각의 feature extractor가 실제 ID와 OOD에 대해 어떠한 성능을 갖는지 확인하였습니다.

![](/assets/images/VennTum/transfer_learning/lpft_1.png)

위 그래프가 앞서 이야기한 linear overparameterized setting에 해당하는 toy example 결과 그래프입니다.
이 때, 앞선 feature extractor를 B, 그리고 해당 downstream task를 풀기 위한 head parameter를 v라고 했을 때, 실제 모델의 전체 parameter는 $w=Bv$ 꼴로 나타나게 됩니다.
그리고 실제 ground truth에 해당하는 optimal w를 $w_{*}$라고 했을 때 위와 같은 양상의 그래프 결과가 나오게 됩니다.

여기에서 확인할 수 있는 점들은 다음과 같습니다.

먼저 linear probing의 경우, pretrained feature extractor B0에 맞춰서 해당하는 head parameter v를 학습하여 $w_{lp}$를 학습하여 만들어냅니다. 이 결과로 기존의 pretrain feature extractor는 해당하는 dataset에 대해 ID인지 OOD인지 여부에 영향받지 않고 각각의 dataset에 대한 정보를 약하게나마 학습한 상태이기 때문에, 해당 B0는 실제 optimal인 $w_{*}$와 OOD accuracy 측면에서 가깝게 align되어 있습니다. 이에 대한 feature extractor 부분은 freeze한 상태로 head parameter만 학습하게 되기 때문에, 그 결과로 생기는 linear probing의 경우 OOD error 측면에서 optimal과 큰 차이가 나지 않습니다.

그러나 이는 fine-tuning에서는 다른 양상을 보이게 됩니다. fine-tuning의 경우 학습 과정에서 feature extractor를 포함한 모든 layer들이 영향을 받고 변화하기 때문에, ID 데이터에 맞추어 fine-tuning하는 과정에서 feature extractor도 ID correction을 높이는 방향으로 변화하게 됩니다. 그 결과로 실제로 OOD와 ID를 모두 고려한 때의 optimal과는 꽤나 각도, 거리가 멀어지게 되며 이 결과 해당 경우의 OOD error는 굉장히 커지게 됩니다.

이러한 현상 속에서, 저자들은 이렇게 fine-tuning 과정에서 feature extractor가 ID direction에 맞추어 update되며 기존의 feature extractor에서 변화하고 ID에 수직으로 큰 오류를 만들어내는 현상을 'distortion'이라 정의합니다.

저자들은 이러한 ft에서의 distortion이 일어날 때, 학습 특정 step t에서의 OOD error lower bound에 대해 다음과 같은 식을 유도해냈습니다.

![](/assets/images/VennTum/transfer_learning/lpft_2.png)
이를 유도하는 자세한 과정은 논문에서 확인하실 수 있습니다. 

여기에서 중요하게 보셔야할 점은, 이러한 lower bound를 구한 결과값에 영향을 주는 요소로 initial head alignment error와 pretrained feature extractor error가 요소로 들어가있다는 것입니다.
여기에서 pretrained feature extractor에 대해서는 우리가 좋은 데이터 셋에 대해 잘 학습하는 것 이외에는 건드릴 수 있는 부분은 없습니다.

그렇다면 우리가 해당 lower bound를 더 줄일 수 있는 부분은 어떤 부분을 수정하는 것일까요?
이에 대한 고려가 바로 LP-FT의 핵심이며, 해당 이야기가 이후에 나올 예정입니다.

### LP vs FT

위 결과를 통해서 fine-tuning을 하는 경우, distortion을 만들어내기 때문에 실제로 학습 과정에서 OOD에 대한 accuracy가 optimal과 큰 격차를 만들어낸다는 것을 확인할 수 있었습니다.
그러나 이러한 결과가 꼭 linear probing이 더 낫다는 것을 의미하지는 않습니다.
그 이유는 linear probing이 OOD 관점에서는 더 우수하지만, 다음과 같은 상황에서 ID accuracy가 더 낮기 때문입니다.

- ID distribution이 lower dimension subspace에 대해 density가 있을 때
- B0가 B*에 가까울 때(즉, 좋은 pretrained feature extractor를 가지고 있을 때)

여기에서 이야기하는 첫 번째 조건은 ID data가 전체 dimension이 d, feature dimension이 k라고 하였을 때, 해당 dataset이 k < m < d-k인 m-dimenstional subspace S에 lying on되어있으며 전체 trainig example n이 m보다 클 때의 경우를 이야기합니다.

반면 OOD에 대해서는 해당하는 subspace S가 B*의 rowspace에 직교하지 않는 한, linear probing이 fine-tuning보다 더 좋은 성능을 갖게 됩니다.

### Extension - Linear probing then fine-tuning: a simple variant to mitigate tradeoffs

앞서 확인한 바들을 따르면, fine-tuning의 장점은 feature extractor와 head가 모두 downstream task에 대해 fit할 수 있다는 것입니다.
이에 저자들은 좋은 pretrained feature를 가지고 있을 때, 이러한 benefit을 그대로 유지하면서 OOD error를 더 낮출 방법을 생각했습니다.

앞서 OOD error lower bound에 대해 이야기할 때, 분자에 initial head alignment error가 중요한 영향을 미치고 있음을 이야기한 적이 있습니다. 지금 현재 가지고 있는 이슈는 FT를 하는 과정에서 head가 random initialization되고 있어서, 해당 error가 보통 굉장히 크다는 것입니다. 그 결과 fine-tuning을 할 때에는 feature extractor와 head가 같이 coupled된 상태로 update되기 때문에, head가 크게 변화하며 fitting되는 과정에서 feature extractor도 distorted하며, OOD error가 커지는 문제가 발생합니다.

이를 해결하기 위해서 우리는 better head initialization이 필요함을 알 수 있습니다.
그리고 저자들은 여기에서 굉장히 좋은 head initializing을 다음에서 발견합니다. 바로 'linear probing'입니다.

Linear probing을 통해 얻은 head는 실제 optimal에 해당하는 v*와 굉장히 가깝게 align되어 있습니다. 이러한 head를 사용하면 실제 학습 과정에서 head의 변동성이 크지 않기 때문에, feature extractor의 distortion도 더욱 작아질 수 있고 OOD error를 낮추는 역할을 할 수 있습니다.

이는 실제 식을 통해서도 구해줄 수 있습니다. 만약 우리가 perfect feature extractor B를 가지고 있을 경우, v0가 최적화된 linear probing head $v_{lp}^{\infty}를 가지게 된다면, 이로 인해 발생하는 feature extractor error와 head initialization error 모두 0이 되어 ft를 통한 OOD error가 모든 step에 대해 0의 lower bound를 가질 수 있게 됩니다.

해당하는 case를 고려하는 것은 이상적인 상황에서 고려된 것이기 때문에, 우리가 perfect feature extractor를 가지고 있지 않을 때와, 무한 step의 linear probing을 진행하지 않은 상태의 linear probing을 사용하였을 때를 analyze하기는 어렵습니다. 그러나 이를 통해서 lower bound가 줄어든다는 것을 확인했기 때문에, 저자들은 다음과 같은 방법을 제안하고 실제 실험을 통해 결과 향상을 확인합니다.

- pretrained feature extractor를 freeze하고 head만 학습하는 linear probing을 몇 step 진행한다.
- 이후 freeze를 푼 이후 feature extractor와 head를 같이 학습하는 fine-tuning을 진행한다.

해당 방법이 바로 저자들이 새롭게 제시하는 fine-tuning 기법, LP-FT입니다.

## Experiments

저자들은 여러 dataset에 대해 실제 LP-FT가 성능을 향상시킬 수 있는지 실험합니다.

이를 위해 사용한 데이터 셋들은 다음과 같습니다.

- DomainNet
- Living-17 and Entity-30
- FMoW Geo-shift
- CIFAR-10 → STL
- CIFAR-10 → CIFAR-10.1
- ImageNet-1K → ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch

이에 실제로 적용할 pretrained model들은 다음과 같습니다. 

- CLIP pretrained ViT-B/16 for ImageNet

이외에도 resnet-50을 사용한 데이터 셋 및 결과 등은 appendix에서 확인하실 수 있습니다.



### Task and evaluation

fine-tuning의 in-distribution, out-of-distribution 성능을 확인하기 위해, 특정 ID 데이터 셋 $P_{ID}$ 에 대한 성능 $L_{ID}$와 해당 ID에서 distribution shift 되어있는 OOD 데이터 셋 $P_{ID}$에 대한 성능 $L_{OOD}$를 측정하여 성능을 비교합니다.

### 


최근에 DNN들은 image를 사용하는 다양한 task에서, 다양한 종류의 data augmentation 기법을 사용하여 성능을 향상시키는 것이 가능했습니다. Data augmentation을 사용하여 실제 사용하는 데이터의 수를 늘리는 방식으로 DNN은 overfitting 문제와 해당하는 train dataset에 대해 memorize하는 문제를 해결하는 것으로 알려져있습니다.
이러한 data augmentation의 경우, 어떠한 comptuer vision task에 대해서도 dataset에 굉장히 쉽게 적용하여 사용할 수 있다는 장점이 있으며, 그 종류에 따라 기존의 모델이 잘 해결하지 못하는 여러 task들 자체를 해결해주는 역할을 할 수 있습니다(이미지 flip, rotation 등).

이러한 여러 data augmentation 방법들 중 일부는 이미지의 일부를 dropout하는 방식으로 구현됩니다. 이미지의 일부를 dropout하여 해당 정보를 의도적으로 손실시켜, 이미지가 다른 영역에 대한 정보를 사용하여 유추할 수 있도록 하는 효과를 볼 수 있으며(Cutout), 어떠한 방법에서는 해당 방식으로 dropout된 영역에 다른 이미지를 채워넣어 두 개의 이미지를 mix하는 방식을 채용하기도 하였습니다(CutMix).

이러한 방법론들은 실제 data augmentation에서 굉장히 높은 성능 향상을 만들어냈습니다.
그러나 이러한 regional dropout 방법론들은 해당 dropout되는 영역에서 인접한 픽셀들이 급격하게 변화하는 'Strong-Edge Problem'을 일으킬 수 있습니다.

결과적으로 이러한 strong-edge problem은 두 가지의 사이드 이펙트를 일으킵니다.

- 픽셀의 급격한 변화가 일어나면 해당 영역에서 local convolution operation에 영향을 준다.
- 이러한 급격한 픽셀 변화가 일어나는 영역은 네트워크가 실제로 잘 캐치할 수 있는 특성이 되기 때문에, 해당 영역에 대해 네트워크의 포커스가 맞춰지게 되며, 이러한 이유로 기존의 dropout 메소드들이 가지고 있던 해당 영역 이외의 정보를 사용하여 추론한다는 기본 전제에 상충된다.

실제로 dropout을 베이스로 하는 data augmentation들에서 이러한 현상이 발생하는지 확인하기 위해, 저자들은 CAM(Class Activation Map)을 통해 학습된 네트워크가 이미지의 어느 영역의 정보를 활용하여 해당 label을 추론하고 있는지 확인합니다.

![](/assets/images/VennTum/data_augmentation/smoothmix_1.png)

이를 확인하는 과정으로, 다음과 같은 모델들에 대한 CAM result를 비교합니다.

- ImageNet을 학습한 기본 ResNet
- Strong-edge based region dropout을 통해 ImageNet을 학습한 ResNet
- Soft-edge based region dropout을 통해 ImageNet을 학습한 ResNet (SmoothMix)

실제 위 figure에서 각각 원본과 해당 순서의 모델들의 CAM result를 확인할 수 있습니다.

여기에서 확인할 수 있는 결과로는, 일단 기본적인 ImageNet에서 학습한 모델의 경우 실제로 이미지에서 어떠한 이미지인지 확인하기 명확한 위치의 region을 보고 추론하고 있다는 것을 알 수 있습니다. 해파리의 경우는 해파리의 형태를 캐치하여 판단하고 있으며, 뱀을 확인하는 과정에서는 뱀의 머리를 위주로 보면서 추론하고 있음을 알 수 있습니다.
이미지를 판단하는 데 있어서 명확한 요소를 보고 판단하고 있으나, 어떻게 보면 해당 영역에 대한 정보만을 위주로 판단하고 있기에, 이미지에서 해당 정보가 손상되어있을 때에 추론이 어려울 수 있습니다.

다음으로 Strong-edge based region dropout을 사용한 모델의 결과에서는 strong-edge를 이미지에 합성해둔 square window에 굉장히 큰 영향을 받고 있다는 것을 알 수 있습니다.
그 예시로, 해파리 사진에서는, 해당 window 내에 해파리에 대한 정보가 존재하지 않음에도 불구하고, 기존의 region dropout을 사용한 이미지를 통해 학습한 경험을 모델이 알고 있기 때문에 해당 window에 집중하여 추론을 적용하고 있음을 알 수 있습니다. 수치 자체가 완전하게 confidence를 나타내는 것은 아니지만, 이를 통해서 해파리가 존재하지 않는 영역을 중심으로 추론하고 있다 보니, 해파리라는 결론을 낼 때의 score가 상당히 낮음을 알 수 있습니다.
이 뿐만아니라, 다른 사진들에서도 만들어둔 window에 초점을 맞추어 해당 영역이 추론에 큰 영향을 미친다는 것을 알 수 있으며, 그 일례로 'Boa constrictor'는 아예 'Green snake'로 분류되었습니다.

그러나 아직 소개되지 않은 SmoothMix를 사용하게 되면, 이러한 square window가 존재하는 상황에서도 해당 영역 이외의 실제로 object에서 중요한 영역을 잘 캐치하여 추론에 사용하고 있으며, 이 뿐만아니라 기존의 baseline resnet보다 더 다양한 영역들을 보면서 추론을 진행하고 있음을 알 수 있습니다.

이제 저자들이 어떻게 Soft-edge based region dropout인 SmoothMix를 구현하였는지 소개하도록 하겠습니다.
  
## Method

### Mask Generation

기본적으로 저자들은 dropout할 영역을 mask로 만들기 위해서 mask generation을 생각합니다.
이에 대해 디테일한 마스크의 생김새와 만드는 방법은 이후에 서술될 예정입니다.

기본적으로, 특정한 random mask를 생성하기 위해, 저자들은 이미지의 너비 W와 높이 H에 대해 uniformly 결정되는 center point를 다음과 같이 구하게 됩니다.

![](/assets/images/VennTum/data_augmentation/smoothmix_2.png)

그리고 이러한 중심점으로부터 얼마나 mask가 퍼져나가는지를 결정하기 위해 $sigma$가 결정되면 이에 맞추어 mask의 크기가 변하게 됩니다.

어떠한 방식을 통해 만들어진 마스크가 G라고 할 때, 각 마스크의 픽셀 단위의 ratio 누적합 $lambda$는 다음과 같이 계산됩니다.

![](/assets/images/VennTum/data_augmentation/smoothmix_3.png)

### Types of Masks

우리는 앞서 저자들이 strong-edge를 사용하여 dropout을 시키면 안된다는 점에 초점을 두었다는 것을 알고 있습니다. 이에 맞추어, SmoothMix는 어떻게 Soft-edge Mask를 만들 것인지에 초점을 둡니다.

이에 저자들은 기본적으로 두 가지 형태의 Mask를 만들어 Soft-edge window를 설정하는 것을 고안합니다.

![](/assets/images/VennTum/data_augmentation/smoothmix_4.png)

첫 번째 방법은 SmoothMix S 방법입니다. 이는 기존의 Cutout, CutMix에서 사용하고 있는 기본적인 square window를 기반으로 하고 있습니다. 해당 square window를 통해 mask를 만들어내지만, 실제로 주변의 바운더리에 대해서만 linear interpolation을 통해 한 번에 0, 1로 나뉘는 것이 아닌, 특졍 0에서 1로 점진적으로 out되는 mask를 만들어줍니다. 이를 통해, 우리는 해당 square window의 boundary에서도 선형적으로 소실되는 형태의 mask를 만들어줄 수 있습니다.

이러한 boundary linear interpolation mask를 만들기 위해, 저자들은 smooth region k에 대해 다음과 같은 형태로 mask를 만들어냅니다.

![](/assets/images/VennTum/data_augmentation/smoothmix_5.png)

두 번째 방법은 SmoothMix C 입니다. 이는 해당 region dropout mask를 만들 때에, Gaussian circle mask를 만드는 것입니다. 특정 중심점으로부터 가우시안 분포를 따르는 형태로 외각으로 가면서 내부로 갈수록 점진적으로 소실되는 형태의 mask를 만들게 됩니다. 이렇게 하면 기존의 square window보다 사람이 보았을 때에도 훨씬 부드럽게 mask가 형성된다는 것을 알 수 있습니다.

이 때, 우리는 생성하는 mask를 아예 원형으로 하는 것이 아닌, 가로와 세로에 해당하는 width와 height에 대해 따로 설정하여 타원 형태의 mask를 만들 수 있습니다. 이는 각각에 대해 사용하는 하이퍼파라미터에 대해 다음과 같이 정의할 수 있습니다.

![](/assets/images/VennTum/data_augmentation/smoothmix_6.png)

이렇게 만들어낸 마스크는 그대로 dropout의 방식을 적용하여 사용할 수 있지만, 저자들은 이에 대해 추가로 해당 마스크에서 dropout되는 영역을 기존의 CutMix에서 사용하는 방식대로 다른 이미지를 넣어 Mixup하는 방식을 사용합니다.

### Blending Images and Labeling

위 과정을 통해 만들어낸 마스크 G에 대해, 우리는 또 다른 이미지를 하나 선택하고, (1 - G)에 해당하는 마스크를 만들어, 각 픽셀의 mask ratio의 합이 1이 되도록하는 새로운 이미지를 생성할 수 있습니다.
이를 생성하는 과정은 기존의 CutMix에서 사용하던 mixup 방식을 그대로 차용할 수 있습니다.

그 결과, 우리가 합성에 사용하려는 이미지가 각각 $(x_{i}, y_{i}), (x_{j}, y_{j})$라고 할 때 새롭게 생성되는 이미지는 다음과 같이 정의할 수 있습니다.

![](/assets/images/VennTum/data_augmentation/smoothmix_7.png)

그리고 이렇게 새롭게 생긴 이미지의 label의 경우는, 앞서 mask의 ratio 누적합을 이용하여 다음과 같이 표현할 수 있습니다.

![](/assets/images/VennTum/data_augmentation/smoothmix_8.png) 

## Experiments

SmoothMix의 평가는 다음과 같은 데이터 셋들에서 이루어집니다.

먼저 가장 기본적인 Baseline reference distribution dataset에 대해 평가를 진행합니다. 해당 데이터셋들로는

- CIFAR-10
- CIFAR-100
- ImageNet

등의 가장 기본적인 데이터 셋들이 사용됩니다.

### Image Classification Results

기본적으로 smoothmix의 image classification 성능을 측정하기 위해, 다음과 같은 다양한 종류의 data augmentation 기법들과 비교를 진행합니다.

- Cutout, Mixup, CutMix, Manifold Mixup, Dropblocks, etc...

이에 대한 기본 baselince 모델로는 PyramidNet-200을 사용하며, SmoothMix S의 경우 k=0.2, $sigma$는 0 과 1 사이에서 유니폼하게 샘플링되며, SmoothMix C의 경우, $sigma$는 0.25~0.5 사이에서 샘플링됩니다.
각각의 데이터 셋에 대해 사용하는 configuration의 경우 상이하게 다를 수 있기 때문에, 해당 디테일은 논문에서 확인해보실 수 있습니다.

![](/assets/images/VennTum/data_augmentation/smoothmix_9.png)

![](/assets/images/VennTum/data_augmentation/smoothmix_10.png)

CIFAR 데이터 셋들에 대한 Top-1, Top-5 error는 다음과 같습니다.
위에서 확인할 수 있듯, SmoothMix 자체로는 기본적인 reference dataset에 대한 성능을 크게 향상시키지는 못했습니다. 기존의 다른 data augmentation 기법들이나 mixup보다는 더 좋은 성능을 보였으나 cutmix보다는 더 좋은 성능을 내지 못했습니다.

![](/assets/images/VennTum/data_augmentation/smoothmix_11.png)

이러한 결과는 ImageNet result에서도 확인할 수 있습니다. 기존의 CIFAR 데이터 셋에서 확인할 수 있던 결과와 마찬가지의 비슷한 양상을 보인다는 것을 알 수 있습니다.

**어떻게 보면 크게 이상한 결과는 아닐수도 있습니다.**
앞서 SmoothMix라는 아이디어를 착안하기 위해서 실험했던 Strong-edge based region dropout과 같은 경우는, **의도적으로 이미지에 square window를 추가하는 adversarial이 가미된 상황** 에서의 결과이기 때문입니다.

우리가 실제로 test data로 사용하는 경우에는 기존의 train dataset과 같은 distribution을 가지는 데이터 중, 그저 unseen dataset만 사용한 경우이기 때문에 이러한 경우에서는 오히려 이미지에서 중요한 요소에 집중하여 모델이 학습하는 것이 더 좋은 결과를 낼 수 있으며, MSDA 등의 기법으로 generalization을 높이는 것으로도 충분할 수 있습니다.

그렇기에, 저자들은 이러한 data augmentation의 reference dataset에 대한 성능을 올리는 것에 중점을 두는 것이 아닌, 다양한 **adversarial cases** 에 초점을 맞추어 실험을 진행합니다.

### Corrupted Image Classification

해당 결과를 소개하기 이전에 corrupted, adversarial case에 대한 경우를 먼저 살펴보도록 하겠습니다.

우리가 특정 reference dataset에 대해 학습하는 경우, 실제로 우리가 이러한 reference distribution에 가까운 데이터들을 수집하여 해당 모델에 사용할 수 있다면, 큰 문제없이 해당 모델들이 주어진 task를 잘 수행할 수 있습니다.
그러나 우리는 의도적으로 손상되거나 적대적인 공격을 받은 데이터 셋이 모델에게 주어질 수 있습니다. 여기에서 adversarial case의 경우는 의도적인 noise 등을 추가하여 의도적으로 모델이 실패하도록 만들기 위한 case들에 해당하기는 하지만, 꼭 이렇게 인위적으로 손상을 일으키는 경우가 아니더라도 data가 손상되는 경우는 다수 존재할 수 있습니다.

예를 들면, 우리가 구할 수 있는 북극곰의 이미지는 많은 경우 빙하가 함께 있거나 눈 덮인 얼음 위의 사진들이 주어지는 경우가 많을 것입니다.
그렇지만 만약 동물원 안에 있는 북극곰 사진이 있을 수도 있고, 혹은 케이지에 넣어서 이동중인 북극곰의 사진이 주어질 수도 있습니다.

또는 어떠한 경우에는 눈이 너무 많이 내리거나 폭풍우가 오는 상황에서의 장미꽃 사진이 주어질 수도 있으며, 50년 전에 찍어두어서 많은 풍화가 일어난 탁자의 사진이 있을 수도 있습니다.

이러한 다양한 자연적인 상황 속에서도 train에서 사용된 reference dataset과는 distribution이 많이 다르거나 손상된 corrupted case들이 존재할 수 있습니다.

많은 경우, 이러한 상황에서 모델의 성능이 떨어지는 것이 잘 알려져있습니다.
또한 data augmentaion을 사용하는 것이, 모델이 주어진 reference dataset 뿐만아니라 distribution shifted case에서도 좀 더 robustness를 갖는다는 것도 알려져있습니다.

그러나 앞서서 저자들이 확인하였던 strong-edge based region dropout과 같은 경우에서, 특정 data augmentation을 사용하였을 때에 추론을 실패하는 failure case가 발생한다는 것을 알 수 있습니다.

이에 대한 관점에서, 저자들은 SmoothMix가 이러한 corrupted case에 대한 robustness를 갖추고 있다고 주장하며 다음 데이터 셋들에 대해 검증을 진행합니다.

- CIFAR-100-Corrupted
- ImageNet-Corrupted

위의 Corrupted dataset들은 기존의 CIFAR, ImageNet Dataset들에 대해 특정한 corrupted type들을 넣어 새롭게 만들어낸 데이터셋들입니다.
이 과정에서 Gaussian noise, snow, motion blur, fog 등등 다양한 종류의 corruption들이 반영되어 있습니다.

![](/assets/images/VennTum/data_augmentation/smoothmix_12.png)

실제 해당 데이터 셋에서 사용하는 corruption의 디테일들은 다음 논문에서 확인해보실 수 있습니다.

[Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations](https://arxiv.org/abs/1807.01697)

이러한 corrupted dataset에서의 result는 다음과 같습니다.

![](/assets/images/VennTum/data_augmentation/smoothmix_13.png)

![](/assets/images/VennTum/data_augmentation/smoothmix_14.png)

위의 table에서 baseline으로 설정되어있는 것은 바로 기본적인 strong-edge based region dropout을 사용한 CutMix 모델입니다. 저자들은 CutMix에서 사용했던 기본 configuration을 유지하여 baseline을 만들고, 이를 SmoothMix와 비교하였습니다.

여기에서 확인할 수 있는 것은 이러한 다양한 종류의 corruption에 대해선 smoothmix가 cutmix보다 더 나은 성능을 보였다는 것입니다. 물론 어느정도는 상이한 부분이 있기는 하지만, Fog등에 대한 case에서는 smoothmix가 cutmix에 비해 굉장히 월등한 성능을 보여주기도 합니다.

굉장히 많은 case에서 성능이 향상된 것을 통해, 저자들은 이러한 smoothmix가 기존의 reference distribution에 대한 성능도 어느정도 유지하면서, 기존의 data augmentation들보다 높은 robustness를 갖춘다고 주장합니다.

## Conclusion

결과적으로 SmoothMix는 strong-edge based DA, MSDA 등을 사용할 때에 해당 edge에 대해 모델이 attention을 갖게 된다는 점을 문제로 삼아, 이러한 단점을 해결할 수 있는 smooth mask를 이용한 augmentation 기법을 제안하였습니다.
실제 실험 결과들로도 이러한 결과가 어느정도 잘 보여졌으며, robustness가 증가했다고 볼 수 있습니다.

**그러나 해당 논문을 실제로 받아들이는 과정에서 꼭 실험을 해보고 확인해보시길 바랍니다**

SmoothMix에서 제안한 방식과 실제로 strong-edge case에서 기본의 data augmentation 기법들이 문제를 가지고 있다는 것은 논문을 통해 잘 알 수 있습니다.
그러나 그렇다고해서 무조건적으로 결과를 수용하기에는 조금 어색한 부분들이 있습니다.

이는 해당 논문이 틀렸다는 것이 아닌, "실제로 결과를 납득하기 위해서는 추가적인 실험 결과들을 확인할 필요가 있다" 정도로 받아들여주시면 좋겠습니다.

1. 먼저, 해당 논문에서 주장한 strong-edge case에 대해서는 data augmentation이 취약한 점을 갖는다는 것을 알 수 있습니다. 그러나 이것은 해당 data augmentation이 가지가 있는 하나의 failure case일 수도 있습니다. 다른 case들에 대해서는 더 좋은 성능을 보일 수도 있고, 혹은 smoothmix가 fail하는 case들이 존재할 수도 있습니다.

2. 사용된 데이터 셋은 CIFAR, ImageNet으로 가장 기본적인 benchmark dataset이기는 하지만, 다양한 종류의 데이터 상황에 대해서도 이러한 경향상이 나타나는지는 미지수입니다. 물론 어느정도 받아들일수는 있으나, 본인이 사용하는 데이터 셋에서도 해당 경향성이 발견되는지는 검증이 필요합니다.

3. Corrupted case들에 대해서 비교하고 검증할 때에는 CutMix 하나만을 baseline으로 잡고 비교하였습니다. 물론, reference distribution에 대해서 가장 높은 성능을 보였던 CutMix였기에 사용한 것으로 볼 수 있습니다. 그러나 다른 data augmentation 기법들(Mixup 등)이 더 높은 robustness를 갖는지 여부 등은 확인해볼 필요가 있습니다. 앞선 SmoothMix의 robustness 측정 결과가 완전한 상위호환도 아니었기 때문에, 이러한 경향성이 실제로 뚜렷한 것인지 검증이 필요합니다.

물론 워크샵 논문이기 때문에 많은 검증이 들어가지 않았고, 분석이 들어가지 않았을 수 있습니다. 그러나 이 과정에서 확인한 strong-edge case에서의 접근이 상당히 좋았기 때문에, 여러 검증이 추가되어 해당 논문을 살펴보시면 많은 도움을 받으실 수 있을 것이라 생각합니다.
