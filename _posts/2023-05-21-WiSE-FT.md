---
layout: post
title:  "WiSE-FT: Robust fine-tuning of zero-shot models (CVPR 2022)"
date:   2023-05-21 08:00:00
author: VennTum
tags: [AI, deep-learning]
---

# [WiSE-FT: Robust fine-tuning of zero-shot models (CVPR 2022)](https://arxiv.org/abs/2109.01903)

본 논문은 대규모 pretrained model에 대한 zero-shot model과 fine-tuning model의 장점을 결합하는 방식인 wiSE-FT를 제안합니다.

이에 대한 더 나은 이해를 위해, 먼저 zero-shot model이 무엇인지에 대해 이야기하고, 해당 논문이 어떠한 방법을 제안하여 해당 문제를 해결하였는지 소개하도록 하겠습니다.

## Zero-shot model

zero-shot이란 모델을 특정 데이터 셋 A에 대해 학습시킨 이후, 이에 대한 다른 추가 train이나 fine-tuning 없이 바로 이와 다른 distribution을 가지거나 혹은 없는 라벨을 포함한 데이터 셋 B에 대해서 inference하는 것을 의미합니다. 결국 간단히 말하자면, zero-shot model은 해당 모델을 학습하는 과정에서 다루지 않았던 라벨이나 데이터, 혹은 더 나아가 다루지 않았던 task에 대해 사용하는 것을 의미하게 됩니다.

보통 일반적인 경우, zero-shot model은 좋은 성능을 내기 어렵습니다. 학습 과정에서 보지 않았던 데이터나 task를 다루기 때문에, 모델은 이것이 무엇인지 파악하기 어렵고 이에 대한 정확도가 많이 낮아지게 됩니다. 그래서 이러한 zero-shot이 가능한 모델들은 일반적으로 굉장히 큰 대규모 데이터 셋에 대해 학습한 모델들을 사용합니다. 많은 양의 데이터들을 학습하는 과정에서, 실제로 해당 데이터 셋에 존재하는 class들에 대한 정보를 잘 가지고 있으며, 또한 만약 inference 과정에서 입력으로 사용된 데이터가 학습 중에 사용된 적 없는 라벨이더라도 다른 라벨들과의 유사도 및 맥락에 대한 이해가 높아 random한 결과보도 훨씬 더 좋은 성능에 그치는 것이 아닌, 유의미한 정도의 성능을 보이는 것이 알려져 있습니다.

### CLIP

이러한 zero-shot model로 사용되는 대표적인 예시로는 바로 [CLIP(2021)](https://arxiv.org/abs/2103.00020) 이 있습니다.
CLIP은 약 4억개에 해당하는 image-text pair 데이터 셋에서 학습된 image-text multimodal task를 수행하는 모델입니다. 가장 기본적인 목적으로는 어떠한 특정 이미지에 대해 잘 설명하는 텍스트 문장을 매치하는 task를 수행하거나, 혹은 어떠한 주어진 텍스트 문장에 가장 잘 일치하는 이미지를 찾아내는 task를 수행합니다.

CLIP을 학습하는 과정은 다음과 같습니다.

- 주어진 데이터 셋에서 N개의 batch를 선택하고, N개의 이미지, 텍스트 라벨들을 모두 pair로 만들어 총 N^2개의 테이블을 만든다.
- N^2개의 테이블에서, 실제 올바른 이미지-텍스트 쌍에 대해서는 cosine similarity가 최대가 되도록 학습하고, 이외의 다른 올바르지 않은 쌍들에 대해서는 cosine similarity가 최소가 되도록 cross-entropy loss를 사용하여 학습한다.

![](/assets/images/VennTum/data_augmentation/wiseft_1.png)

이와 같이 학습된 CLIP 모델의 경우, 이미지와 텍스트 사이의 상관관계에 대한 정보를 잘 학습하게 됩니다. 그 결과로 어떠한 텍스트 맥락이 어떠한 이미지를 의미하는지, 어떠한 이미지가 어떠한 텍스트 맥락으로 해석되는지에 대한 유사성을 잘 파악하게 됩니다.

이렇게 학습된 CLIP의 경우, 이미지-텍스트 multimodal이 가능하고, 입력으로 다양한 형태로 넣어줄 수 있기 때문에 여러 태스크에 사용하는 것이 가능합니다.

이 중 우리는 오늘 이야기할 WiSE-FT와 관련 있는 zero-shot inference에 대해 이야기해보도록 하겠습니다.

### Zero-shot inference of CLIP

CLIP에서 zero-shot inference를 하는 과정은 다음과 같습니다.
CLIP을 학습할 때 사용하지 않은, unseen 데이터 셋에 대해 특정 이미지가 어떠한 object인지를 classification 하고싶은 경우, 다음과 같이 동작합니다.

- inference를 통해 확인하고 싶은 object를 image 하나로 선정하여 CLIP의 image encoder에 넣는다.
- unseen 데이터 셋에 존재하는 모든 label들에 대해, label text로 만들어서 CLIP의 text encoder에 넣는다.
- image encoder와 text encoder에서 나온 모든 pair들에 대해 cosine similarity가 가장 큰 label text에 해당하는 label을 inference result로 사용한다.

![](/assets/images/VennTum//wiseft_2.png)

특정 text에 대응하는 image를 찾아내고 싶은 경우, 이와 반대로 image encoder에 후보 image들을 넣어서 similarity를 구할 수 있습니다.

이와 같은 방식으로 동작하는 CLIP은 놀랍게도 unseen image와 label들에 대해서도 굉장히 잘 동작합니다.
그 이유는 CLIP이 대규모 dataset에 대해 pretrained되는 과정에서 text와 image의 semantic information에 대해 굉장히 잘 이해하고 있어서, 실제로 확인하지 않았던 label에 대해서도 그것이 어떠한 semantic을 가지고 있는지 유추할 수 있다는 것입니다. 이러한 이유로, CLIP은 zero-shot에서도 상당히 잘 동작하는 결과를 보입니다.

## Abstract

그러나 이러한 CLIP의 경우, 아무리 zero-shot에 대해 잘 동작한다 하더라도 해당하는 unseen dataset에 대해 transfer learning을 진행하는 것보다는 성능이 더 낮습니다.
아무래도 해당하는 데이터를 확인한다면, 해당하는 target distribution을 잘 캐치할 수 있게 되며, 이 과정에서 정확도가 더 높아지게 됩니다.

하지만, 많은 경우에 zero-shot 모델과 transfer learning을 통한 fine-tuning에 대해 큰 문제점이 드러나는 부분이 있습니다.
그리고 이 부분이 본 논문에서 저자들이 태클하는 문제 상황이 됩니다.

본 논문에서 저자들은 CLIP과 같은 대규모 pretrained model을 fine-tuning하여 사용하는 과정에서 발생하는 distribution shift에 취약하다는 점을 지적합니다.

많이 알려진 사실로, CLIP과 같은 zero-shot model들은 distribution shift에 대해 robustness를 갖는다는 점이 있습니다.
예를 들어, zero-shot으로 레몬 이미지를 판단하게 된다면 해당하는 이미지가 실물 이미지이든, 플라스틱 모형이든, 그림이든 어느정도 해당 이미지가 레몬이라는 것을 잘 파악하게 됩니다.

우리가 실제로 사용하려는 이미지가 실물 이미지라면, 해당하는 실물 이미지 셋에 대해 fine-tuning 혹은 linear probing을 거쳐서 target distribution을 학습하게 되면 실물 이미지에 대해 더욱 더 잘 판단하게 됩니다. 그러나 이와 같이 특정 target distribution에 대해 fine-tuning을 거치게 되면, target distribution에 대해서는 굉장히 높은 정확도 향상이 일어나지만, 같은 라벨이지만 약간의 distribution shift가 일어난 다른 dataset에 대해서는 오히려 zero-shot의 경우보다 정확도가 더 떨어지게 됩니다.

이러한 이유로, zero-shot 모델은 robustness 관점에서 좋고, fine-tuning model은 target accuracy 관점에서 더 좋은 성능을 낸다는 성질이 있습니다.

여기에서 저자들은 어떻게 하면 zero-shot 모델의 robustness를 갖추고, fine-tuning의 target accuracy를 갖출 수 있는지에 대해 고민하고, 이를 해결하기 위해 wiSE-FT라는 방법을 시도합니다.

## Method

### Prior Experiment

실제 dataset을 사용하여 정량적으로 분석하기에 앞서, ImageNet-9를 기반으로 생성한 합성 데이터들을 ImageNet에 pretrained된 ResNet-50 모델에 넣었을 때에, 이들 label을 어떻게 분류하는지에 대해 간단한 실험 결과를 확인할 수 있습니다.

![](/assets/images/VennTum/data_augmentation/noise_or_signal_1.png)

Figure 1을 보면 다음과 같은 8가지의 합성을 통해서 모델이 해당 이미지의 label을 올바르게 맞추는지 검증합니다.

- 실제 곤충 이미지
- 곤충의 background 이미지만 사용하고, foreground 영역은 검정으로 지움
- 곤충의 background 이미지를 사용하고, foreground 영역은 background로 채움
- 곤충의 background 이미지만 사용하고, foreground object는 검정으로 지움
- 곤충의 foreground 이미지만 사용하고, foreground obejct 이외에는 검정으로 지움
- 곤충의 foreground 이미지만 사용하고, background 영역은 다른 곤충 이미지의 background로 채움
- 곤충의 foreground 이미지만 사용하고, background 영역은 다른 라벨 이미지의 background로 채움
- 곤충의 foreground 이미지만 사용하고, background 영역은 다음 라벨 이미지의 background로 채움

이에 대한 결과로 실제 맞추게 된 결과는 figure 1에 나온 것처럼, 절반의 task에서 실패한다는 것을 확인할 수 있습니다.
이 중에서 실제로 foreground가 존재함에도 불구하고 실패하는 5, 8번의 경우도 존재합니다. 또한 어떻게 본다면, 4번 task도 실제 이미지에서 foreground의 영역 contour 자체는 나비 모양으로 생겼지만, 결과는 bird로 예측하는 2번 task와 다르지 않다는 것도 확인할 수 있습니다.

위 figure가 보여주는 background와 model prediction 사이의 관계는 다음으로 설명할 수 있습니다.

먼저, 2번과 4번 task에서는 foreground가 존재하지 않았음에도 불구하고 bird라는 label을 predict 했습니다. 이는 많은 경우에, 나무나 풀, 그리고 하늘이 있는 배경에서 새가 존재했기 때문에 생긴 bias로 볼 수 있습니다. 이 과정에서 foreground 자체만 확인했을 때에는 label을 확인하기 어려우니 background information에 맞춰서 편향된 것입니다.

그리고 5, 8번의 경우에는 온전히 foreground가 존재함에도 불구하고 나비인 insect로 분류하지 않은 이유를 굉장히 잘 확인할 수 있습니다. 이들 task에서 분류된 라벨은 instrument입니다. 이는 많은 경우, 악기들이 배경이 갈색이나 어두운 배경에서 찍힌 사진들로 구성되어있기 때문에, 검정색이라는 배경에 bias를 가져 이처럼 분류했음을 생각해볼 수 있습니다.

이제부터는 실제 많은 다른 label들에 대해서도 이러한 background information이 model에 얼마나 많은 영향을 주는지 확인하기 위해서 정량적인 분석을 위한 실험을 시작합니다.

### Methodology

실제로 모델에서 image background information이 어떤 영향을 주는지 확인하기 위해서 ImageNet-9로부터 새로운 합성 데이터셋을 만듭니다.

- Base Dataset: ImageNet-9를 base dataset으로 사용합니다. ImageNet-9는 기존의 ImageNet에서 9개의 coarse-grained classes만을 사용한 데이터 셋입니다. 이를 만들어내기 위해서, 저자들은 ImageNet의 annotated bounding box가 존재하는 데이터들만 사용하여 실제 이미지의 foreground와 background을 분리하여 variation을 만들 수 있는 이미지들만 사용하여 각 label들에 대해 5045개의 training dataset과 450개의 test image들을 만들었습니다.
- Varations of ImageNet-9: 기존의 ImageNet-9에서 bounding box information을 사용하여, figure 1에서 선보여진 7개 종류의 variation을 만들어서 실험을 진행합니다.
- Lager Dataset: ImageNet-9L은 실제 모델을 학습하는 과정에서 모델의 generalization을 올리기 위해서, bounding box가 존재하지 않은 이미지들을 모두 포함한 총 9개의 라벨에 대한 ImageNet의 부분집합 데이터 셋입니다. 이를 사용하여 실제 모델을 학습시킵니다.

![](/assets/images/VennTum/data_augmentation/noise_or_signal_2.png)

위 Table 1은 앞서 제가 figure 1에서 설명했던, 각 variation들이 어떤 식으로 만들어졌는지 설명하는 내용과 동일합니다.

이러한 데이터 셋들을 사용하여, 이젠 실제로 background signal이 어떠한 영향을 주는지 실험합니다.

## Quantifying Reliance on Background Signals

### Backgrounds suffice for classification

이번 실험에서는 background만 존재하는 데이터들만 사용하여 모델을 학습하고, 그 결과를 확인합니다. 여기에서 사용되는 variation들은 앞선 figure 1에서 이야기된 2, 3, 4번에 해당되는 경우들입니다. 

![](/assets/images/VennTum/data_augmentation/noise_or_signal_3.png)

실제 실험 결과를 figure 2를 통해 확인하게 되면, 주어진 train dataset에서는 어떠한 foreground도 존재하지 않았지만, 이들만 사용한 데이터를 통해 학습한 모델이 모두 40% 이상의 정확도를 보여준다는 것을 확인할 수 있습니다. 전체 label이 9개가 존재한다고 생각하면, 이는 절대 random한 결과(random이면 약 11%의 정확도를 보여야 함)를 보이는 것이 아닌, 실제 background information만을 활용해도 test dataset을 꽤나 높은 확률로 맞출 수 있다는 결과를 보여줍니다.

이는 결과적으로, train dataset에서 가지고 있는 background information과 실제 test dataset에 존재하는 background information이 높은 correlation을 가지고 있다는 것으로 요약할 수 있습니다. 이러한 correlation을 가지고 있기 때문에, 실제로 foreground에 대한 정보를 모르고도, background를 사용하여 inference를 하는 것이 가능했습니다.

### Models exploit background signal for classification

![](/assets/images/VennTum/data_augmentation/noise_or_signal_4.png)

이번 실험에서는 foreground와 background를 실제 같은 이미지가 다닌 다른 이미지들끼리 섞어서 합성한 데이터 세트에 대해 test할 때에 어떠한 현상이 일어나는지 확인합니다.

Table 2를 확인하게 되면, 이번 실험에서는 2개의 모델을 사용하게 됩니다. 하나는 기존의 ImageNet에서 pretrained된 다양한 모델들, 또 다른 하나는 ImageNet-9L을 사용하여 학습한 모델들이 존재합니다. 그리고 실제 이들을 original imagenet, ImageNet-9, ImageNet-9L, 그리고 variation들에 대한 test를 할 때에 accuracy가 어떻게 되는지 확인할 수 있습니다.

결과를 통해 알 수 있는 점은, Only-BG-T의 경우처럼 background info만 존재하는 경우에도 상당히 모델이 잘 맞췄다는 것을 확인할 수 있는 점과(앞선 실험과 비슷한 결론), Mixed variation의 경우, 실제로 기존의 foreground는 전혀 건드리지 않고 똑같이 존재함에도 불구하고, background information이 변했다는 이유로 정확도가 낮아진다는 것입니다.

이 중에서도, 같은 label을 가지는 mixed-same보다 다른 label의 background를 가질 수도 있는 mixed-rand의 정확도가 더 낮다는 것을 통해, background information에 의해서 실제로 모델의 오분류할 가능성이 커진다는 것을 확인할 수 있습니다.

즉, 이 말은 곧 모델이 어떠한 object를 추정하는 과정에서 주어진 이미지의 background information 또한 함께 사용하여 추론하게 된다는 것입니다. 그러한 과정에서 background information에 대한 bias를 얻게 되어 큰 영향을 미칠 수도 있게 됩니다.

### Models are vulnerable to adversarial backgrounds

Background information이 model에 얼마나 안좋은 영향을 미치는지 확인하기 위해서, 이번에는 적대적으로 선택된 background에 대한 모델의 robustness를 측정합니다.
이를 수행하기 위해서, 저자들은 각각의 foreground에 대해, 이 foreground를 다른 background와 조합했을 때에 해당 이미지를 background label로 추정하게 하는 foreground가 얼마나 많은지 확인합니다. 이를 한 결과, 총 87.5%의 foreground들이 자신과 매치했을 때, background로 예측하게 하는 background가 존재한다는 것을 확인할 수 있습니다.

![](/assets/images/VennTum/data_augmentation/noise_or_signal_5.png)

figure 3의 경우, 보여지는 이미지들은 모두 꽃을 background로 가지고 있는, 실제 원본이 insect가 아닌 이미지들입니다. 이 중에서 label에 해당하는 foreground를 background로 채우는 adversarial backgrounds가 적용되었습니다. 그 결과로 실제 이 라벨을 가지는 이미지의 background information은 높은 확률로 insects로 분류될 정도로 높은 bias를 준다는 것을 확인할 수 있습니다.

![](/assets/images/VennTum/data_augmentation/noise_or_signal_6.png)

figure 4는 반대로, 실제 insect 이미지의 background에 다른 label의 foreground를 가지는 이미지를 분류하는 과정에서, 실제 foreground를 insect로 분류하는 데 성공한 success rate와 그 counts입니다. 즉, 이는 다른 라벨의 foreground를 가지고도, 주어진 이미지가 background에 해당하는 insect로 분류하는 경우가 꽤 많이 존재한다는 것을 보여주어, adversarial하게 background에 영향을 받음을 확인할 수 있습니다.

### Training on MIXED-RAND reduces background dependence

이번에는 foreground와 background의 label이 다른 경우인 Mixed-Rand가 얼마나 background dependence를 낮출 수 있는지에 대한 실험입니다. 이는 Mixed-Rand dataset으로 학습한 모델을 가지고 실제 여러 variation들에 대해 얼마나 robust한 test accuracy를 보이는지 확인합니다.

![](/assets/images/VennTum/data_augmentation/noise_or_signal_7.png)

실제 위 figure 5를 확인하게 되면, foreground와 background를 mix한 dataset을 사용하는 것으로 variation들에 대해 평균적인 accuracy가 높고 그 편차가 크지 않다는 것을 확인할 수 있습니다. 이는 실제로 이러한 데이터 셋에서 학습된 모델의 경우, background robustness가 증가한다는 것을 확인할 수 있는 부분입니다.

![](/assets/images/VennTum/data_augmentation/noise_or_signal_8.png)

이는 실제로 모델이 바라보게 되는 saliency map을 통해서 확인할 수 있습니다. mixed-rand model의 경우, 기존의 original보다 foreground에 더 집중해서 분류하고 있다는 것을 알 수 있습니다.

**그러나 여기에서 조금 더 생각하게 되면, mixed-rand dataset을 통해 학습할 경우, 기존의 original data에 대해서는 accuracy가 떨어지는 것을 확인할 수 있습니다.**

우리는 앞선 robustness를 증가하는 것 이외에도, 왜 이러한 현상이 일어났는지에 대해 살펴볼 필요가 있습니다. 우리가 여기에서 추론할 수 있는 결과로는, 결국에 background information에 robust한 모델이 실제로는 original dataset에서 성능이 떨어진다는 사실을 확인할 수 있습니다.
이는 어떻게 보면, 실제 data의 경우, background information을 활용하는 것이 주어진 foreground를 추론하는 과정에서 꽤나 효과적인 bias라는 것을 생각해볼 수 있습니다.

예를 들면, 우리가 어떠한 object가 A인지 B인지 foreground만 보고는 알기 어려운 경우를 생각해보겠습니다. 근데 만약, 여기에서 우리가 A는 물에서 살고, B는 땅에서 산다는 사실을 알고 있다면 어떻게 될까요? 우리는 헷갈리는 와중에도, 배경을 보고 만약 물일 경우 A로, 땅일 경우 B로 추론을 할 수 있을 것입니다.

**이러한 경우가 바로 background information이 추론 과정에서 좋은 방향의 bias를 주는 경우로 생각해볼 수 있습니다.**

## Benchmark Progress and Background Dependence

이번에는 실제로 기본적인 computer vision benchmark에서 exploiting background correlation으로부터 robust해지기 위해서 어떠한 방식들이 존재해야하는지에 대한 실험을 공유합니다.

![](/assets/images/VennTum/data_augmentation/noise_or_signal_9.png)

figure 8은 범례에 존재하는 여러 variation dataset들에 대해, 실제 ImageNet accuracy와 이러한 synthetic dataset에 대한 accuracy를 그린 그림입니다. 여기에서 확인할 수 있는 것은, ImageNet에서 학습한 모델의 accuracy가 높으면 높을수록, 실제 background adversarial한 attack에 대한 accuracy 또한 높아진다는 것입니다.

그러나 Only-BG-T와 같은 경우를 보면, background information만 존재하는 경우에도 label 분류 정확도가 함께 올라간다는 것을 확인할 수 있습니다. 이는 앞서 이야기한 것처럼, 모델의 성능이 증하갈 때에 background information 활용 또한 함께 올라간다는 것과, 이에 대한 bias를 같이 갖추게 된다는 것을 의미합니다. 이는 결국, 어떻게 보면 background informatino이 실제 inference 과정에서 큰 도움을 주고 있으며 이러한 정보가 존재해야만 높은 정확도를 가질 수 있다는 것을 시사하는 것으로 볼 수 있습니다.

## Conclusion

결과적으로 우리는 classifier가 얼마나 이미지 background에 의존적인지에 대한 연구를 확인할 수 있었습니다.
그 결과로 모델은 adversarial한 경우나, 혹은 평균적인 경우에도 background에 robust하지 않다는 것을 확인할 수 있으며, 모델이 이러한 background를 활용한다는 것을 알 수 있습니다.
이는 실제로 모델에 대한 공격으로 background adversarial attack등이 들어올 때에 모든 이미지의 87.5% 정도가 이러한 background change에 의해 속을 수 있다는 것을 의미합니다.

그러나 많은 경우에, background information은 그 자체로도 모델이 정확한 추론을 하기 위해 많은 도움을 줄 수 있습니다. 비록 그 정보 자체가 bias에 의한 것일 수 있지만, 인간의 시각 처리와 비슷하게 background에서 정보를 추출하면 만약 주어진 foreground가 흐릿하거나 왜곡되는 등의 손상을 입은 경우에도, 어렵지 않게 추론하는 것이 가능할 수 있습니다.

그리고 해당 논문에서 제시한 foreground-background mixing 방법을 통해서, 이러한 adversarial한 attack에 robust한 classfier를 만드는 방법을 제시하여 더욱 robustness를 올리는 것도 가능함을 알 수 있습니다.

## Personal Opinion

앞선 논문을 통해서 알 수 있는 것과, 실제 여러가지 데이터 셋을 통해 경험한 것은 바로, 모델이 이러한 background robustness가 절대 높지 않다는 것이며, 그리고 augmentation 등의 method를 사용할 때에 이러한 종류의 손상이 존재하지 않는지 고려해봐야한다는 것입니다.

실제로 medical dataset을 사용한 경우 중 기존의 data augmentation 기법이나, 혹은 이전에 소개했었던 simple copy-and-paste와 같은 기법으로 augmentation을 할 때에 오히려 성능이 감소하는 경우도 존재했었다는 것을 들은 적이 있고, 어떠한 task에 사용되느냐에 따라서 data augmentation 적용 여부가 달라지기도 한다는 점이 이러한 background information에 영향을 받은 경우일 수도 있다는 생각을 합니다.

항상 많은 논문들을 리뷰하면서도 작성하는 내용이지만, 언제나 논문들은 주어진 환경과, 많은 경우 정제된 benchmark dataset을 이용한다는 점을 늘 염두해두고 있어야 합니다. 이러한 것을 고려해서, 어떠한 요소들이 부작용을 주는지, 혹은 줄 수 있는지를 미리 생각해보고 적용하게 된다면 많은 경우에서 원인을 빠르게 찾을 수 있을 것입니다.










